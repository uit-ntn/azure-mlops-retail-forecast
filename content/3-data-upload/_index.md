---
title: "Data Upload & Storage Configuration"
date: 2025-08-30T11:20:00+07:00
weight: 3
chapter: false
pre: "<b>3. </b>"
---

Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω thi·∫øt l·∫≠p Azure Storage Account v√† upload d·ªØ li·ªáu ecommerce dataset v√†o container `data/raw/ecommerce/`. ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu cho qu√° tr√¨nh training v√† validation trong Azure ML pipeline. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng Azure Portal UI v√† AzCopy ƒë·ªÉ upload c√°c file CSV.gz v·ªõi t·ªïng dung l∆∞·ª£ng ‚â•4GB.

## 1. M·ª•c ti√™u

‚úÖ **T·∫°o Azure Storage Account v·ªõi hierarchical namespace (ADLS Gen2)**  
‚úÖ **Upload dataset 2019-*.csv.gz v√†o container data/raw/ecommerce/**  
‚úÖ **ƒê·∫£m b·∫£o d·ªØ li·ªáu s·∫µn s√†ng cho Azure ML training**  
‚úÖ **C·∫•u h√¨nh proper access permissions v√† security**

## 2. Dataset Overview

**Retail E-commerce Dataset (2019)**
- **Source**: Kaggle E-commerce Dataset
- **Format**: CSV files compressed v·ªõi gzip (.csv.gz)
- **Time Range**: 2019 (12 th√°ng d·ªØ li·ªáu)
- **Size**: ‚â•4GB t·ªïng dung l∆∞·ª£ng
- **Structure**: Transaction data v·ªõi product, user, category information

<div style="background: linear-gradient(135deg, #e6f3ff 0%, #dbeafe 100%); border: 1px solid #bfdbfe; border-left: 6px solid #0078d4; padding: 18px; margin: 18px 0; border-radius: 10px;">
  <strong>M·ª•c ti√™u</strong>: T·∫£i c√°c file th√°ng 2019-*.csv.gz v√†o Storage data/raw/ecommerce/. UI: Azure Portal / Storage Explorer (ho·∫∑c AzCopy). B·∫±ng ch·ª©ng: ·∫¢nh data/raw/ecommerce/ hi·ªÉn th·ªã c√°c .csv.gz (‚â•4GB t·ªïng). Done: D·ªØ li·ªáu s·∫µn s√†ng ƒë·ªçc.
</div>

## 3. AZURE STORAGE SETUP

### 3.1 Storage Account Configuration

<div style="background: #f8fafc; border: 1px solid #e2e8f0; border-left: 4px solid #0078d4; padding: 20px; margin: 20px 0; border-radius: 8px;">
<strong>üîß Storage Account Settings</strong>

**1. Basic Configuration:**
- **Storage account name**: `retailforecastdev` (unique globally)
- **Performance**: Standard (cost-effective cho large datasets)
- **Redundancy**: LRS (Locally-redundant storage)
- **Location**: Southeast Asia (matching Resource Group)

**2. Advanced Features:**
- **Hierarchical namespace**: ‚úÖ Enabled (ADLS Gen2)
- **Access tier**: Hot (frequent access)
- **Blob soft delete**: ‚úÖ Enabled (7 days retention)
- **Versioning**: ‚úÖ Enabled (data protection)
</div>

### 3.2 UI Flow (Azure Portal)

**Step 1: Create Storage Account**
```
Azure Portal ‚Üí Storage accounts ‚Üí Create storage account
```

**Step 2: Basic Configuration**
- **Subscription**: [Your subscription]
- **Resource group**: `retail-dev-rg`
- **Storage account name**: `retailforecastdev`
- **Region**: Southeast Asia
- **Performance**: Standard
- **Redundancy**: Locally-redundant storage (LRS)

**Step 3: Advanced Settings**
- **Hierarchical namespace**: ‚úÖ Enable (ADLS Gen2)
- **Access tier**: Hot
- **Blob soft delete**: ‚úÖ Enable (7 days)
- **Versioning**: ‚úÖ Enable

**Step 4: Networking & Security**
- **Network access**: Allow access from all networks (dev environment)
- **Secure transfer required**: ‚úÖ Enabled
- **Allow Blob public access**: ‚ùå Disabled (security)

### 3.3 Container Structure Setup

Sau khi t·∫°o Storage Account, t·∫°o container structure:

```bash
# Container hierarchy
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ ecommerce/          # Raw CSV.gz files
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ features/           # Processed features
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ artifacts/          # Model artifacts
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ training/           # Training logs
```

## 4. DATA UPLOAD METHODS

### 4.1 Method 1: Azure Portal (Storage Explorer)

**Advantages**: User-friendly, visual interface, drag-and-drop
**Best for**: Small datasets, manual uploads, testing

**Steps:**
1. Navigate to Storage Account ‚Üí Containers
2. Create container `data` (if not exists)
3. Create folder structure: `raw/ecommerce/`
4. Upload CSV.gz files via drag-and-drop
5. Verify file sizes and permissions

### 4.2 Method 2: AzCopy (Recommended)

**Advantages**: High performance, resume capability, batch operations
**Best for**: Large datasets, automation, production uploads

```bash
# Install AzCopy (if not installed)
# Windows
winget install Microsoft.AzCopy

# macOS
brew install azcopy

# Linux
wget https://aka.ms/downloadazcopy-v10-linux
tar -xzf downloadazcopy-v10-linux.tar.gz
sudo ./install.sh
```

**Authentication Setup:**
```bash
# Login to Azure
az login

# Get storage account key
STORAGE_KEY=$(az storage account keys list \
  --resource-group retail-dev-rg \
  --account-name retailforecastdev \
  --query '[0].value' -o tsv)

# Set environment variable
export AZCOPY_STORAGE_KEY=$STORAGE_KEY
```

**Upload Commands:**
```bash
# Upload single file
azcopy copy "2019-Jan.csv.gz" \
  "https://retailforecastdev.dfs.core.windows.net/data/raw/ecommerce/2019-Jan.csv.gz" \
  --recursive=false

# Upload multiple files (batch)
azcopy copy "./datasets/" \
  "https://retailforecastdev.dfs.core.windows.net/data/raw/ecommerce/" \
  --recursive=true \
  --include-pattern="2019-*.csv.gz"

# Upload with progress and resume capability
azcopy copy "./datasets/" \
  "https://retailforecastdev.dfs.core.windows.net/data/raw/ecommerce/" \
  --recursive=true \
  --include-pattern="2019-*.csv.gz" \
  --log-level=INFO \
  --overwrite=prompt
```


## 5. SECURITY & ACCESS CONTROL

### 5.1 RBAC Permissions

**Storage Account Level:**
- **ML Engineers**: `Storage Blob Data Contributor`
- **Data Scientists**: `Storage Blob Data Reader`
- **DevOps**: `Storage Account Contributor`
- **Azure ML Workspace MI**: `Storage Blob Data Reader`

### 5.2 Access Policies

```bash
# Assign Storage Blob Data Reader to Azure ML Managed Identity
az role assignment create \
  --assignee <aml-workspace-managed-identity-id> \
  --role "Storage Blob Data Reader" \
  --scope "/subscriptions/<subscription-id>/resourceGroups/retail-dev-rg/providers/Microsoft.Storage/storageAccounts/retailforecastdev"

# Assign Storage Blob Data Contributor to ML team
az role assignment create \
  --assignee <ml-team-object-id> \
  --role "Storage Blob Data Contributor" \
  --scope "/subscriptions/<subscription-id>/resourceGroups/retail-dev-rg/providers/Microsoft.Storage/storageAccounts/retailforecastdev"
```

### 5.3 Network Security

**Development Environment:**
- Allow access from all networks (for development)
- Enable secure transfer (HTTPS only)
- Disable public blob access

**Production Considerations:**
- Private endpoints for secure access
- VNet integration
- IP whitelisting for specific ranges

## 6. DATA VALIDATION

### 6.1 File Verification

```bash
# List uploaded files
az storage blob list \
  --account-name retailforecastdev \
  --container-name data \
  --prefix "raw/ecommerce/" \
  --output table

# Check file sizes
az storage blob list \
  --account-name retailforecastdev \
  --container-name data \
  --prefix "raw/ecommerce/" \
  --query "[].{Name:name, Size:properties.contentLength}" \
  --output table

# Calculate total size
az storage blob list \
  --account-name retailforecastdev \
  --container-name data \
  --prefix "raw/ecommerce/" \
  --query "[].properties.contentLength" \
  --output tsv | awk '{sum += $1} END {print "Total size:", sum/1024/1024/1024, "GB"}'
```

### 6.2 Data Quality Checks

```python
# Python script to validate uploaded data
from azure.storage.filedatalake import DataLakeServiceClient
from azure.identity import DefaultAzureCredential
import pandas as pd
import gzip

def validate_uploaded_data():
    credential = DefaultAzureCredential()
    service_client = DataLakeServiceClient(
        account_url="https://retailforecastdev.dfs.core.windows.net",
        credential=credential
    )
    
    file_system_client = service_client.get_file_system_client("data")
    directory_client = file_system_client.get_directory_client("raw/ecommerce")
    
    # List files
    files = list(directory_client.list_paths())
    
    print(f"üìÅ Found {len(files)} files:")
    total_size = 0
    
    for file_path in files:
        file_client = directory_client.get_file_client(file_path.name)
        properties = file_client.get_file_properties()
        size_mb = properties.size / (1024 * 1024)
        total_size += properties.size
        
        print(f"  ‚úÖ {file_path.name}: {size_mb:.2f} MB")
        
        # Quick validation - try to read first few rows
        try:
            download = file_client.download_file()
            with gzip.open(download, 'rt') as f:
                # Read first 5 lines
                for i, line in enumerate(f):
                    if i >= 5:
                        break
                print(f"    üìä File format valid")
        except Exception as e:
            print(f"    ‚ùå Error reading file: {e}")
    
    total_gb = total_size / (1024 * 1024 * 1024)
    print(f"\nüìà Total dataset size: {total_gb:.2f} GB")
    
    if total_gb >= 4.0:
        print("‚úÖ Dataset size requirement met (‚â•4GB)")
    else:
        print("‚ö†Ô∏è  Dataset size below requirement (<4GB)")

if __name__ == "__main__":
    validate_uploaded_data()
```

## 7. B·∫∞NG CH·ª®NG HO√ÄN TH√ÄNH

### 7.1 ·∫¢nh 1: Storage Account Overview
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üìã Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ Storage Account overview page</li>
<li>‚úÖ Account name: <code>retailforecastdev</code></li>
<li>‚úÖ Status: <code>Active</code></li>
<li>‚úÖ Hierarchical namespace: <code>Enabled</code></li>
<li>‚úÖ Access tier: <code>Hot</code></li>
</ul>
</div>

### 7.2 ·∫¢nh 2: Container Structure
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üìÅ Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ Container <code>data</code> created</li>
<li>‚úÖ Folder structure: <code>raw/ecommerce/</code></li>
<li>‚úÖ All CSV.gz files visible</li>
<li>‚úÖ File sizes displayed</li>
</ul>
</div>

### 7.3 ·∫¢nh 3: File Details & Total Size
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üìä Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ List of all 2019-*.csv.gz files</li>
<li>‚úÖ Individual file sizes visible</li>
<li>‚úÖ Total size calculation (‚â•4GB)</li>
<li>‚úÖ Upload timestamps</li>
</ul>
</div>

### 7.4 ·∫¢nh 4: Access Control (IAM)
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üîê Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ Storage Account ‚Üí Access control (IAM)</li>
<li>‚úÖ Role assignments visible</li>
<li>‚úÖ Azure ML Managed Identity permissions</li>
<li>‚úÖ Team member permissions</li>
</ul>
</div>

### 7.5 ·∫¢nh 5: AzCopy Upload Progress
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üì§ Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ AzCopy command execution</li>
<li>‚úÖ Upload progress showing files being transferred</li>
<li>‚úÖ Transfer speed and completion percentage</li>
<li>‚úÖ Final success message</li>
</ul>
</div>

### 7.6 ·∫¢nh 6: Storage Account Properties
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>‚öôÔ∏è Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ Storage Account ‚Üí Properties</li>
<li>‚úÖ Hierarchical namespace: Enabled</li>
<li>‚úÖ Access tier: Hot</li>
<li>‚úÖ Replication: Locally-redundant storage (LRS)</li>
<li>‚úÖ Secure transfer required: Enabled</li>
</ul>
</div>

### 7.7 ·∫¢nh 7: Data Validation Results
<div style="background: #fef2f2; border: 1px solid #fecaca; border-left: 4px solid #ef4444; padding: 15px; margin: 15px 0; border-radius: 8px;">
<strong>üìä Y√™u c·∫ßu screenshot:</strong>
<ul>
<li>‚úÖ Python validation script output</li>
<li>‚úÖ File count and individual sizes</li>
<li>‚úÖ Total size calculation (‚â•4GB)</li>
<li>‚úÖ File format validation results</li>
</ul>
</div>

## 8. TI√äU CH√ç HO√ÄN TH√ÄNH

### 8.1 Functional Requirements
- [x] **Storage Account created**: `retailforecastdev` v·ªõi ADLS Gen2 enabled
- [x] **Container structure**: `data/raw/ecommerce/` folder created
- [x] **Files uploaded**: All 2019-*.csv.gz files uploaded successfully
- [x] **Size requirement**: Total dataset size ‚â•4GB
- [x] **Access permissions**: Proper RBAC configured

### 8.2 Data Quality Requirements
- [x] **File integrity**: All files uploaded without corruption
- [x] **Format validation**: CSV.gz format verified
- [x] **Accessibility**: Files accessible by Azure ML workspace
- [x] **Security**: Secure transfer enabled, public access disabled

### 8.3 Performance Requirements
- [x] **Upload performance**: Efficient upload method used (AzCopy recommended)
- [x] **Storage tier**: Hot access tier for frequent access
- [x] **Network optimization**: Optimal region selection (Southeast Asia)

## 9. AUTOMATION SCRIPTS

### 9.1 Complete Upload Script

```bash
#!/bin/bash
# upload-dataset.sh

# Configuration
STORAGE_ACCOUNT="retailforecastdev"
RESOURCE_GROUP="retail-dev-rg"
CONTAINER_NAME="data"
DATASET_PATH="./datasets"
TARGET_PATH="raw/ecommerce"

echo "üöÄ Starting dataset upload to Azure Storage..."

# Get storage account key
echo "üìã Getting storage account key..."
STORAGE_KEY=$(az storage account keys list \
  --resource-group $RESOURCE_GROUP \
  --account-name $STORAGE_ACCOUNT \
  --query '[0].value' -o tsv)

if [ -z "$STORAGE_KEY" ]; then
    echo "‚ùå Failed to get storage account key"
    exit 1
fi

# Create container if not exists
echo "üìÅ Creating container if not exists..."
az storage container create \
  --account-name $STORAGE_ACCOUNT \
  --account-key $STORAGE_KEY \
  --name $CONTAINER_NAME

# Upload files using AzCopy
echo "üì§ Uploading files using AzCopy..."
azcopy copy "$DATASET_PATH/" \
  "https://$STORAGE_ACCOUNT.dfs.core.windows.net/$CONTAINER_NAME/$TARGET_PATH/" \
  --recursive=true \
  --include-pattern="2019-*.csv.gz" \
  --log-level=INFO

# Verify upload
echo "‚úÖ Verifying upload..."
az storage blob list \
  --account-name $STORAGE_ACCOUNT \
  --account-key $STORAGE_KEY \
  --container-name $CONTAINER_NAME \
  --prefix "$TARGET_PATH/" \
  --output table

# Calculate total size
echo "üìä Calculating total size..."
TOTAL_SIZE=$(az storage blob list \
  --account-name $STORAGE_ACCOUNT \
  --account-key $STORAGE_KEY \
  --container-name $CONTAINER_NAME \
  --prefix "$TARGET_PATH/" \
  --query "[].properties.contentLength" \
  --output tsv | awk '{sum += $1} END {print sum/1024/1024/1024}')

echo "üìà Total dataset size: ${TOTAL_SIZE} GB"

if (( $(echo "$TOTAL_SIZE >= 4.0" | bc -l) )); then
    echo "‚úÖ Dataset upload completed successfully!"
    echo "‚úÖ Size requirement met (‚â•4GB)"
else
    echo "‚ö†Ô∏è  Warning: Dataset size below requirement (<4GB)"
fi
```

### 9.2 Terraform Configuration

```hcl
# storage.tf
resource "azurerm_storage_account" "main" {
  name                     = "retailforecastdev"
  resource_group_name      = azurerm_resource_group.main.name
  location                 = azurerm_resource_group.main.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  account_kind             = "StorageV2"
  
  # Enable ADLS Gen2
  is_hns_enabled = true
  
  # Security settings
  min_tls_version                 = "TLS1_2"
  allow_nested_items_to_be_public = false
  
  # Enable versioning and soft delete
  versioning_enabled = true
  blob_properties {
    delete_retention_policy {
      days = 7
    }
    versioning_enabled = true
  }
  
  tags = {
    Environment        = "development"
    Project           = "retail-forecast"
    Purpose           = "data-storage"
    DataClassification = "internal"
  }
}

resource "azurerm_storage_container" "data" {
  name                  = "data"
  storage_account_name  = azurerm_storage_account.main.name
  container_access_type = "private"
}
```

## 10. L∆ØU √ù QUAN TR·ªåNG

### 10.1 Cost Optimization
<div style="background: #fffbeb; border: 1px solid #fed7aa; border-left: 4px solid #f59e0b; padding: 20px; margin: 20px 0; border-radius: 8px;">
<strong>üí∞ Storage Cost Considerations</strong>
<ul>
<li><strong>Access Tier</strong>: Hot tier for frequent access during development</li>
<li><strong>Lifecycle Management</strong>: Consider Cool/Archive tiers for older data</li>
<li><strong>Compression</strong>: CSV.gz already compressed, good for storage efficiency</li>
<li><strong>Redundancy</strong>: LRS sufficient for development, consider GRS for production</li>
</ul>
</div>

### 10.2 Data Governance
- **Data Lineage**: Track data sources and transformations
- **Retention Policies**: Implement data retention rules
- **Access Logging**: Monitor data access patterns
- **Compliance**: Ensure data handling meets regulatory requirements

### 10.3 Next Steps
1. ‚úÖ **Data Upload completed** ‚Üê Current step
2. üîÑ **Azure ML Workspace setup** ‚Üê Next step
3. üîÑ **Data Asset registration**
4. üîÑ **Training pipeline configuration**
5. üîÑ **Model training execution**

{{% notice info %}}
**Best Practice**: Use AzCopy for large file uploads (>100MB) as it provides better performance, resume capability, and progress tracking compared to Azure Portal uploads.
{{% /notice %}}

{{% notice warning %}}
**Security Note**: Never store storage account keys in code or configuration files. Use Azure Key Vault or Managed Identity for secure access in production environments.
{{% /notice %}}

## 11. USEFUL LINKS

- **Azure Storage Documentation**: https://docs.microsoft.com/en-us/azure/storage/
- **AzCopy Documentation**: https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10
- **ADLS Gen2 Best Practices**: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices
- **Storage Security Guide**: https://docs.microsoft.com/en-us/azure/storage/common/security-recommendations

Data upload ho√†n t·∫•t! üéâ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho **Task 4: Azure ML Workspace Setup**.
